# Timeline

* **[Week of 2/18]: Theme - Warmup** 
	* Surveyed materials, warm-up (fast.ai, CS 231N)
	* Created Github [Learning Project](https://github.com/robert8138/deep-learning-deliberate-practice)

* **[Week of 2/25]: Theme - Mathematical Formulation of Neural Nets**
	* Warm-up continued
	* Coursera DL course 1 (Basic of NN)
	* Studied backward propogation ([here](http://neuralnetworksanddeeplearning.com/chap2.html), [here](http://colah.github.io/posts/2015-08-Backprop/), and [here](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)).

* **[Week of 3/4]: Theme - Convolutional Neural Network**
	* Keras Book Chapter 1,2,4, and 5 (CNN) 
	* Finished Coursera DL course 2 (Optimization/Hyperparameter Tuning)
	* Study CNN in details (Stanford CS 231N [CovNet Notes](http://cs231n.github.io/convolutional-networks/) being the most useful)

* **[Week of 3/11]: Theme - Convolutional Neural Network (Interleaving)**
	* CS 231 coverage on Neural Network formulation
	* Finished Coursera DL course 4 (CNN)

* **[Week of 3/18]: Theme - Additional Topics in CNN / Hands-on Coding**
	* CS 231N (CNN Architecture, Visualizing CNN)
	* Deep Learning Software, Running Keras code on Redspot
	* Finished Coursera DL course 3 and started meta-topics on structuring ML projects

* **[Week of 3/25]: Theme - Hands-on Coding / Practical Advice for ML/DL**
	* Keras Book Chapter 7 on Functional API
	* Practical Tips for ML projects from Ian Goodfellow
	* Started and completed investigation about Image pipelines & CNN at Airbnb
	* Discovered Stanford CS 230
	* Finally read through [Rules of ML: Best Practice for ML Engineering](https://developers.google.com/machine-learning/rules-of-ml/)

* **[Week of 4/1]: Theme - Recurrent Neural Network & NLP**: I am a little bit hesitant to dig too deep in NLP just yet. It's a completely different topic and I still feel I haven't fully grasp CNN, so I will treat this as a quick survey for now. I think the proper way to learn it is to take [CS 224N](http://web.stanford.edu/class/cs224n/index.html).
	* Stanford CS 231N revisit Lecture 10 on RNN
	* Keras Book Chapter 6 on Recurrent Neural Network & RNN using Keras
	* Started Coursera DL course 5 - RNN
	* [CS 244N Word2Vec lecture notes](http://web.stanford.edu/class/cs224n/lectures/lecture2.pdf)

* **[Week of 4/8]: Theme - Recurrent Neural Network & NLP (Interleaving)**: to reinforce all the basic things I learned about word embeddings, RNNs from Coursera DL in Stanford class setting again
	* Stanford CS 224N Lecture 2: Word2Vec -> word embeddings
	* Stanford CS 224N Lecture 3: Co-ocurrence, GloVec word embeddings, BLEU scores
	* Stanford CS 224N Lecture 8: Vanilla RNN, Bidirectional RNN, Deep RNN
	* Stanford CS 224N Lecture 9: Gated Recurrent Units (GRU), Long-Short Term Memory (LSTM)
	* [Edwin Chen's explanation on LSTM with visualization](http://blog.echen.me/2014/05/30/exploring-lstms/)
	* [The Unreasonable Effectiveness of Recurrent Neural Network](http://karpathy.github.io/2015/05/21/rnn-effectiveness/): char RNN model -> {Paul Graham essays, Shakespeare, LaTex, C, Linux}

# Upcoming

* [Week of 4/15]: Unsupervised Learning, Generative Models, Autoencoders ... etc 